## August 12 - August 25:

Build more knowledge about LLM

### Completion:

* reading through llm basics

* read through some LangChain tutorials and some documentations



## August 26 - September 1:

Build a functional LLM with RAG and benchmark the model

### completion: 

RAG(Gemma 9b) and user defined character traits

### Todo:

* benchmark performance: ?
* exame LangSmith outputs



## September 2 - September 22:

Apply Finetuning techniques and model compression to the current selected model

* research on different fine tuning techniques and compare the difference
* model compression(quantization) seems trivial



## September 16 - September 29

Add more functions to the model as needed



## September 30 - October 27

Search for best possible deployment solution, switch model and fine tuning method as needed



## October 28 -

Train the final model and benchmark

